{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fdf1cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_path_neg = \".........................................\"\n",
    "file_path_pos = \".........................................\"\n",
    "# Define the path to the output file\n",
    "output_file_path = \"......................................\\\\vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d81eabc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def clean_text(text, min_freq=2):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove accented characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = filtered_tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Remove words with low frequency\n",
    "    word_counts = Counter(lemmatized_tokens)\n",
    "    filtered_tokens = [token for token in lemmatized_tokens if word_counts[token] > min_freq]\n",
    "    \n",
    "    text = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcd2eff1",
   "metadata": {},
   "source": [
    "# Create a list to store the cleaned texts\n",
    "cleaned_texts = []\n",
    "\n",
    "# Loop through the files in folder1\n",
    "for filename in os.listdir(file_path_neg):\n",
    "    file = open(os.path.join(file_path_neg, filename), 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    cleaned_text = clean_text(text)\n",
    "    cleaned_texts.append(cleaned_text)\n",
    "\n",
    "# Loop through the files in folder2\n",
    "for filename in os.listdir(file_path_pos):\n",
    "    file = open(os.path.join(file_path_pos, filename), 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    cleaned_text = clean_text(text)\n",
    "    cleaned_texts.append(cleaned_text)\n",
    "\n",
    "# Combine the cleaned texts into one string\n",
    "combined_text = \"\\n\".join(cleaned_texts)\n",
    "\n",
    "# Write the combined text to the output file\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "74e4dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0350f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# retrieve the raw text \n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "# tokenize the raw text\n",
    "tokens = word_tokenize(vocab)\n",
    "# count the frequency of occurrences\n",
    "fdist = FreqDist(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "64a93740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152341\n",
      "[('film', 10318), ('movie', 5885), ('one', 4533), ('character', 2629), ('like', 2297), ('scene', 1541), ('get', 1503), ('time', 1477), ('even', 1259), ('story', 1238), ('make', 1222), ('good', 1207), ('life', 888), ('would', 879), ('also', 833), ('much', 803), ('see', 787), ('two', 710), ('way', 686), ('first', 684), ('really', 628), ('thing', 621), ('well', 607), ('bad', 601), ('go', 596), ('action', 584), ('plot', 557), ('know', 556), ('year', 551), ('people', 525), ('he', 508), ('man', 507), ('love', 481), ('take', 480), ('new', 466), ('little', 465), ('alien', 462), ('u', 457), ('performance', 452), ('never', 437), ('could', 432), ('world', 423), ('many', 421), ('come', 409), ('work', 402), ('great', 394), ('star', 387), ('show', 384), ('best', 381), ('actor', 376)]\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "# Print top 50 most word and its frequency\n",
    "print(fdist.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4cd1fdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(vocab.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "74d56fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "77e331e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in os.listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '\\\\' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4edeb6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    neg = process_docs(file_path_neg, vocab, is_train)\n",
    "    pos = process_docs(file_path_pos, vocab, is_train)\n",
    "    docs = neg + pos\n",
    "    # prepare labels\n",
    "    labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "    return docs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d954966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0f2ae012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " 'plot two teen go church party drink drive get accident one girlfriend see life whats deal watch movie find movie teen generation cool idea bad package review even harder one write since generally attempt break head lost highway memento there good bad making didnt one seem taken pretty concept movie well main problem simply normal fantasy world as audience member idea whats going there there coming back dead there others who look like dead there strange there there chase there weird happen simply explained dont mind trying film every give clue get kind biggest problem obviously got big secret seems want completely final five make entertaining thrilling even engaging really sad part arrow like actually point start make little bit sense still didnt make film entertaining guess bottom line like always make sure audience even given secret enter world mean showing melissa sagemiller running away throughout movie okay get there people chasing dont know who really need see giving us different offering going movie apparently studio took film away director there pretty decent teen movie here guess decided music video little edge would make sense pretty good part although wes bentley seemed be playing exact character he american beauty new neighborhood biggest go sagemiller who throughout entire film actually feeling overall film doesnt stick doesnt confusing pretty despite pretty cool ending explanation oh way horror teen slasher flick look way someone apparently genre still also production two ago ever since whatever skip coming nightmare elm street blair witch crow crow lost highway memento others stir')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain[0],train_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3e45696c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length: 914\n"
     ]
    }
   ],
   "source": [
    "# calculate the maximum sequence length\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "print('Maximum length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "61480e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8666aa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokenizer`\n",
    "tokenizer = create_tokenizer(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "422a404e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5552\n"
     ]
    }
   ],
   "source": [
    "# define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8fbfc319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# integer encode and pad documents\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    # pad sequences\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cb9d35f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode data\n",
    "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
    "Xtest = encode_docs(tokenizer, max_length, test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1e380f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense \n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# define the model\n",
    "def define_model(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ee5ff460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 914, 100)          555200    \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 907, 32)           25632     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 453, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 14496)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                144970    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 725,813\n",
      "Trainable params: 725,813\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = define_model(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aeaa909c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 - 6s - loss: 0.6918 - accuracy: 0.5372 - 6s/epoch - 105ms/step\n",
      "Epoch 2/10\n",
      "57/57 - 5s - loss: 0.5966 - accuracy: 0.6844 - 5s/epoch - 87ms/step\n",
      "Epoch 3/10\n",
      "57/57 - 5s - loss: 0.4237 - accuracy: 0.9056 - 5s/epoch - 91ms/step\n",
      "Epoch 4/10\n",
      "57/57 - 5s - loss: 0.3499 - accuracy: 0.9600 - 5s/epoch - 90ms/step\n",
      "Epoch 5/10\n",
      "57/57 - 5s - loss: 0.3115 - accuracy: 0.9839 - 5s/epoch - 91ms/step\n",
      "Epoch 6/10\n",
      "57/57 - 5s - loss: 0.2885 - accuracy: 0.9883 - 5s/epoch - 91ms/step\n",
      "Epoch 7/10\n",
      "57/57 - 5s - loss: 0.2727 - accuracy: 0.9922 - 5s/epoch - 88ms/step\n",
      "Epoch 8/10\n",
      "57/57 - 5s - loss: 0.2590 - accuracy: 0.9944 - 5s/epoch - 89ms/step\n",
      "Epoch 9/10\n",
      "57/57 - 5s - loss: 0.2482 - accuracy: 0.9933 - 5s/epoch - 88ms/step\n",
      "Epoch 10/10\n",
      "57/57 - 5s - loss: 0.2370 - accuracy: 0.9944 - 5s/epoch - 96ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d8047727d0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "22722a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f44187c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 99.44%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on training dataset\n",
    "loss, acc = model.evaluate(Xtrain, ytrain, verbose=0)\n",
    "print('Train Accuracy: %.2f%%' % (acc * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2bc59db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 83.00%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test dataset\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %.2f%%' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cbea8028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
    "    # clean review\n",
    "    line = clean_doc(review, vocab)\n",
    "    # encode and pad review\n",
    "    padded = encode_docs(tokenizer, max_length, [line])\n",
    "    # predict sentiment\n",
    "    yhat = model.predict(padded, verbose=0)\n",
    "    # retrieve predicted percentage and label\n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos) == 0:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    return percent_pos, 'POSITIVE'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39494a32",
   "metadata": {},
   "source": [
    "## Truncated Sequences:\n",
    "#### Truncate reviews to a mean length of review lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "beef39d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length: 244\n"
     ]
    }
   ],
   "source": [
    "# calculate the mean sequence length\n",
    "mean_length = int(sum([len(s.split()) for s in train_docs]) / len(train_docs))\n",
    "print('Mean length: %d' % mean_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c130d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode and pad documents\n",
    "def encode_docs_mean(tokenizer, mean_length, docs):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    # pad sequences\n",
    "    padded = pad_sequences(encoded, maxlen=mean_length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e1e2f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode data\n",
    "Xtrain_mean = encode_docs_mean(tokenizer, mean_length, train_docs)\n",
    "Xtest_mean = encode_docs_mean(tokenizer, mean_length, test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7e78cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# define the model\n",
    "def define_model_mean(vocab_size, mean_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=mean_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model_mean.png', show_shapes=True)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d2466405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 244, 100)          555200    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 237, 32)           25632     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 118, 32)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3776)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                37770     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 618,613\n",
      "Trainable params: 618,613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_mean = define_model_mean(vocab_size, mean_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ddf67c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 - 3s - loss: 0.6919 - accuracy: 0.5261 - 3s/epoch - 44ms/step\n",
      "Epoch 2/10\n",
      "57/57 - 2s - loss: 0.5677 - accuracy: 0.7844 - 2s/epoch - 38ms/step\n",
      "Epoch 3/10\n",
      "57/57 - 2s - loss: 0.1970 - accuracy: 0.9406 - 2s/epoch - 36ms/step\n",
      "Epoch 4/10\n",
      "57/57 - 2s - loss: 0.0327 - accuracy: 0.9972 - 2s/epoch - 32ms/step\n",
      "Epoch 5/10\n",
      "57/57 - 2s - loss: 0.0077 - accuracy: 1.0000 - 2s/epoch - 38ms/step\n",
      "Epoch 6/10\n",
      "57/57 - 2s - loss: 0.0031 - accuracy: 1.0000 - 2s/epoch - 34ms/step\n",
      "Epoch 7/10\n",
      "57/57 - 2s - loss: 0.0016 - accuracy: 1.0000 - 2s/epoch - 37ms/step\n",
      "Epoch 8/10\n",
      "57/57 - 2s - loss: 0.0010 - accuracy: 1.0000 - 2s/epoch - 36ms/step\n",
      "Epoch 9/10\n",
      "57/57 - 2s - loss: 6.5521e-04 - accuracy: 1.0000 - 2s/epoch - 36ms/step\n",
      "Epoch 10/10\n",
      "57/57 - 3s - loss: 4.6226e-04 - accuracy: 1.0000 - 3s/epoch - 49ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d87b38c610>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit network\n",
    "model_mean.fit(Xtrain_mean, ytrain, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b8c94b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_mean.save('model_mean.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d9b3e1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy for mean length: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on training dataset\n",
    "loss_train_mean, acc_train_mean = model_mean.evaluate(Xtrain_mean, ytrain, verbose=0)\n",
    "print('Train Accuracy for mean length: %.2f%%' % (acc_train_mean * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f2997310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 81.50%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test dataset\n",
    "loss_test_mean, acc_test_mean = model_mean.evaluate(Xtest_mean, ytest, verbose=0)\n",
    "print('Test Accuracy: %.2f%%' % (acc_test_mean*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d471aebe",
   "metadata": {},
   "source": [
    "## Truncated Vocabulary: \n",
    "#### We removed infrequently occurring words, but still had a large vocabulary of more than 25,000 words. Perform further reducing the size of the vocabulary and the effect on model skill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "174bee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc_new(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c26d1230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs_new(directory, vocab, is_train):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in os.listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '\\\\' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc_new(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7ee5b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_dataset_new(vocab, is_train):\n",
    "    # load documents\n",
    "    neg = process_docs_new(file_path_neg, vocab, is_train)\n",
    "    pos = process_docs_new(file_path_pos, vocab, is_train)\n",
    "    docs = neg + pos\n",
    "    # prepare labels\n",
    "    labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "    return docs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "51404fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs_new, ytrain_new = load_clean_dataset_new(vocab, True)\n",
    "test_docs_new, ytest_new = load_clean_dataset_new(vocab, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8beaa240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length: 226\n"
     ]
    }
   ],
   "source": [
    "# calculate the mean sequence length\n",
    "mean_length_new = int(sum([len(s.split()) for s in train_docs_new]) / len(train_docs_new))\n",
    "print('Mean length: %d' % mean_length_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8f4595db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokenizer`\n",
    "tokenizer = create_tokenizer(train_docs_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "49c1a0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New vocabulary size: 5543\n"
     ]
    }
   ],
   "source": [
    "# define vocabulary size\n",
    "vocab_size_new = len(tokenizer.word_index) + 1\n",
    "print('New vocabulary size: %d' % vocab_size_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6dfdafe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode and pad documents\n",
    "def encode_docs_new(tokenizer, mean_length_new, docs):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    # pad sequences\n",
    "    padded = pad_sequences(encoded, maxlen=mean_length_new, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ae5efa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode data\n",
    "Xtrain_new = encode_docs_new(tokenizer, mean_length_new, train_docs_new)\n",
    "Xtest_new = encode_docs_new(tokenizer, mean_length_new, test_docs_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e020287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model_new(vocab_size_new, mean_length_new):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size_new, 100, input_length=mean_length_new))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model_new.png', show_shapes=True)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "849a1c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 226, 100)          554300    \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 219, 32)           25632     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 109, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 3488)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                34890     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 614,833\n",
      "Trainable params: 614,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_new = define_model_new(vocab_size_new, mean_length_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "eff870ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 - 3s - loss: 0.6909 - accuracy: 0.5394 - 3s/epoch - 44ms/step\n",
      "Epoch 2/10\n",
      "57/57 - 2s - loss: 0.5294 - accuracy: 0.8183 - 2s/epoch - 38ms/step\n",
      "Epoch 3/10\n",
      "57/57 - 2s - loss: 0.1524 - accuracy: 0.9489 - 2s/epoch - 37ms/step\n",
      "Epoch 4/10\n",
      "57/57 - 2s - loss: 0.0272 - accuracy: 0.9972 - 2s/epoch - 32ms/step\n",
      "Epoch 5/10\n",
      "57/57 - 2s - loss: 0.0059 - accuracy: 1.0000 - 2s/epoch - 39ms/step\n",
      "Epoch 6/10\n",
      "57/57 - 2s - loss: 0.0028 - accuracy: 1.0000 - 2s/epoch - 33ms/step\n",
      "Epoch 7/10\n",
      "57/57 - 2s - loss: 0.0017 - accuracy: 1.0000 - 2s/epoch - 36ms/step\n",
      "Epoch 8/10\n",
      "57/57 - 2s - loss: 0.0012 - accuracy: 1.0000 - 2s/epoch - 35ms/step\n",
      "Epoch 9/10\n",
      "57/57 - 2s - loss: 8.8626e-04 - accuracy: 1.0000 - 2s/epoch - 33ms/step\n",
      "Epoch 10/10\n",
      "57/57 - 2s - loss: 7.0310e-04 - accuracy: 1.0000 - 2s/epoch - 39ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d820db57e0>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit network\n",
    "model_new.fit(Xtrain_new, ytrain_new, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "27070c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_new.save('model_new.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "dfa4fbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on training dataset\n",
    "loss_train_new, acc_train_new = model_new.evaluate(Xtrain_new, ytrain_new, verbose=0)\n",
    "print('Train Accuracy: %.2f%%' % (acc_train_new * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fc57538f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 81.50%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test dataset\n",
    "loss_test_new, acc_test_new = model_new.evaluate(Xtest_new, ytest_new, verbose=0)\n",
    "print('Test Accuracy: %.2f%%' % (acc_test_new*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3e6eb6",
   "metadata": {},
   "source": [
    "## Use Embedding:\n",
    "#### Experiment loading the pre-trained GloVe or any other pretrained embedding and the impact on model skill with and without further fine-tuning during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a3358d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings into dictionary\n",
    "glove_embeddings = {}\n",
    "with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d70f9752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix from GloVe embeddings\n",
    "embedding_matrix = np.zeros((vocab_size_new, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "09495e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model_glove(vocab_size_new, mean_length_new, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size_new, 100, input_length=mean_length_new, \n",
    "                        weights=[embedding_matrix], trainable=False))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model_glove.png', show_shapes=True)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cadaa0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 226, 100)          554300    \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 219, 32)           25632     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 109, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 3488)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                34890     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 614,833\n",
      "Trainable params: 60,533\n",
      "Non-trainable params: 554,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_glove = define_model_glove(vocab_size_new, mean_length_new, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "22d0e5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 - 2s - loss: 0.6939 - accuracy: 0.5033 - 2s/epoch - 39ms/step\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.6932 - accuracy: 0.4894 - 944ms/epoch - 17ms/step\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.6931 - accuracy: 0.5006 - 932ms/epoch - 16ms/step\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.6931 - accuracy: 0.4822 - 1s/epoch - 18ms/step\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.6931 - accuracy: 0.5000 - 1s/epoch - 24ms/step\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.6930 - accuracy: 0.5000 - 943ms/epoch - 17ms/step\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.6930 - accuracy: 0.4850 - 920ms/epoch - 16ms/step\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.6925 - accuracy: 0.4822 - 930ms/epoch - 16ms/step\n",
      "Epoch 9/10\n",
      "57/57 - 2s - loss: 0.6927 - accuracy: 0.5011 - 2s/epoch - 32ms/step\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 0.6925 - accuracy: 0.4922 - 1s/epoch - 23ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d8023d44f0>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit network\n",
    "model_glove.fit(Xtrain_new, ytrain_new, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b89b9a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_glove.save('model_glove.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c55f0c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on training dataset\n",
    "loss, acc = model_glove.evaluate(Xtrain_new, ytrain_new, verbose=0)\n",
    "print('Train Accuracy: %.2f%%' % (acc * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "82d5223b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test dataset\n",
    "loss, acc = model_glove.evaluate(Xtest_new, ytest_new, verbose=0)\n",
    "print('Test Accuracy: %.2f%%' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f63c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
